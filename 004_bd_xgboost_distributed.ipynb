{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"004_bd_xgboost_distributed.ipynb","provenance":[{"file_id":"1E-V6ATR2LcgGUHBJG-d4kcNHXK-FCMvB","timestamp":1638280394363}],"collapsed_sections":["rCC2aZDLKEUo","scfLT2i0MLyD","gaoyQqOHUZxa","OKhcbM9IEwDY","s88i61HALTSd"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"rCC2aZDLKEUo"},"source":["# XGBoost -- Distributed Computing with Dask and Rapids"]},{"cell_type":"markdown","metadata":{"id":"JRBxwdHkKNgw"},"source":["In this notebook, we look at the wildly different results, as well as syntax, required when XGBoost is trained in a (simulated) distributing computing environment.\n","\n","While it is possible to train XGBoost using distributed CPU resources only with vanilla Dask, in this notebook we will be demonstrating NVIDIA's RAPIDS. \n","\n","Just as Dask extends the Pandas dataframe into a distributed CPU environment, Rapids extends the Pandas dataframe into a distributed GPU environment by storing tabular data in columnar PyArrow format across an arbitrary number of GPUs.\n","\n","Unlike PyTorch, XGBoost has no native support for this kind of distributed processing, and so it falls to these specialist frameworks to handle most of the heavy lifting.\n","\n","RESOURCES AND DATASETS\n","\n","[NVIDIA XGBoost/CuDF Walkthrough](https://developer.nvidia.com/blog/accelerating-xgboost-on-gpu-clusters-with-dask/)\n","\n","[Dask-ml Documentation](https://ml.dask.org/)"]},{"cell_type":"markdown","metadata":{"id":"scfLT2i0MLyD"},"source":["# Environment Sanity Check #\n","\n","Click the _Runtime_ dropdown at the top of the page, then _Change Runtime Type_ and confirm the instance type is _GPU_.\n","\n","Check the output of `!nvidia-smi` to make sure you've been allocated a Tesla T4, P4, or P100."]},{"cell_type":"code","metadata":{"id":"B0C8IV5TQnjN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638280464077,"user_tz":300,"elapsed":453,"user":{"displayName":"Ben Feuer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfEIGd7cuEAlEsO0rgrPNGu5w5YPlSFdk8U8iseac=s64","userId":"08479976376697798693"}},"outputId":"2d527b8b-45ed-4266-ce60-c13d33713101"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tue Nov 30 13:54:23 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   44C    P0    29W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"markdown","metadata":{"id":"CtNdk7PSafKP"},"source":["#Setup xgboost, dask-ml, RAPIDS\n","\n","Unfortunately, setting up Dask and RAPIDS in Colab is quite a chore. That said, if you follow the instructions in the cells below, you'll be up and running in about a half an hour. \n","\n","Expect to have to monitor the first 5-10 minutes, as there will be multiple restarts required. You also lose access to the handy file browser included in Colab, because in order to run RAPIDS, you actually have to change the environment of the underlying cloud server to Conda, because RAPIDS doesn't support pip installation.\n","\n","Procedure:\n","1. Update gcc in Colab\n","1. Install Conda\n","1. Install RAPIDS' current stable version of its libraries, as well as some external libraries including:\n","  1. cuDF\n","  1. cuML\n","  1. cuGraph\n","  1. cuSpatial\n","  1. cuSignal\n","  1. BlazingSQL\n","  1. xgboost\n","1. Copy RAPIDS .so files into current working directory, a neccessary workaround for RAPIDS+Colab integration.\n","5. Install additional modules (dask-ml, pytorch lightning) using pip to avoid extremely slow conda environment checks. The modules will be visible in conda, as will the native Colab modules.\n","\n","Overall, expect the installs to take 20-30 minutes -- **they will also require multiple restarts of the Colab instance**, so if you are running this on your own, be prepared to monitor this process pretty closely. Once the conda installs are running, you should be OK, unless you have changed something elsewhere in the environment."]},{"cell_type":"code","metadata":{"id":"3Jeh6EJBaBkv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638733786441,"user_tz":300,"elapsed":1149,"user":{"displayName":"Ben Feuer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfEIGd7cuEAlEsO0rgrPNGu5w5YPlSFdk8U8iseac=s64","userId":"08479976376697798693"}},"outputId":"c25d29fa-50a4-478d-e702-c71ed679ccf3"},"source":["# This get the RAPIDS-Colab install files and test check your GPU.  Run this and the next cell only.\n","# Please read the output of this cell.  If your Colab Instance is not RAPIDS compatible, it will warn you and give you remediation steps.\n","!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n","!python rapidsai-csp-utils/colab/env-check.py"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'rapidsai-csp-utils' already exists and is not an empty directory.\n","***********************************************************************\n","Woo! Your instance has the right kind of GPU, a Tesla P100-PCIE-16GB!\n","***********************************************************************\n","\n"]}]},{"cell_type":"code","metadata":{"id":"JI7UTXbhaBon","colab":{"base_uri":"https://localhost:8080/"},"outputId":"56c7d4d2-1159-4e3c-ae51-8697042cad8e"},"source":["# This will update the Colab environment and restart the kernel.  Don't run the next cell until you see the session crash.\n","!bash rapidsai-csp-utils/colab/update_gcc.sh\n","import os\n","os._exit(00)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Updating your Colab environment.  This will restart your kernel.  Don't Panic!\n","Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n","Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","Hit:3 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n","Get:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n","Hit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n","Get:6 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n","Ign:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","Hit:8 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n","Hit:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n","Hit:10 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Hit:13 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n","Get:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n","Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","Hit:16 http://ppa.launchpad.net/ubuntu-toolchain-r/test/ubuntu bionic InRelease\n","Fetched 256 kB in 2s (109 kB/s)\n","Reading package lists... Done\n","Added repo\n","Hit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n","Ign:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","Ign:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n","Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Hit:6 http://archive.ubuntu.com/ubuntu bionic InRelease\n","Hit:7 http://security.ubuntu.com/ubuntu bionic-security InRelease\n","Hit:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n","Hit:11 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n","Hit:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n","Hit:13 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n","Hit:14 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n","Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","Hit:16 http://ppa.launchpad.net/ubuntu-toolchain-r/test/ubuntu bionic InRelease\n","Reading package lists... Done\n","Installing libstdc++\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","libstdc++6 is already the newest version (11.1.0-1ubuntu1~18.04.1).\n","Selected version '11.1.0-1ubuntu1~18.04.1' (Toolchain test builds:18.04/bionic [amd64]) for 'libstdc++6'\n","0 upgraded, 0 newly installed, 0 to remove and 63 not upgraded.\n","restarting Colab...\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nVRZnKCQJYPV","executionInfo":{"status":"ok","timestamp":1638733924605,"user_tz":300,"elapsed":9457,"user":{"displayName":"Ben Feuer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfEIGd7cuEAlEsO0rgrPNGu5w5YPlSFdk8U8iseac=s64","userId":"08479976376697798693"}},"outputId":"3405f2d5-7932-4006-80c5-fc372365b0f9"},"source":["!pip install dask-ml rasterio ipython-autotime pymongo[tls,srv]\n","%load_ext autotime"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting dask-ml\n","  Using cached dask_ml-2021.11.30-py3-none-any.whl (148 kB)\n","Collecting rasterio\n","  Using cached rasterio-1.2.10-cp37-cp37m-manylinux1_x86_64.whl (19.3 MB)\n","Collecting ipython-autotime\n","  Using cached ipython_autotime-0.3.1-py2.py3-none-any.whl (6.8 kB)\n","Collecting pymongo[srv,tls]\n","  Downloading pymongo-4.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n","\u001b[K     |████████████████████████████████| 452 kB 4.1 MB/s \n","\u001b[?25hCollecting scikit-learn>=1.0.0\n","  Downloading scikit_learn-1.0.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (23.2 MB)\n","\u001b[K     |████████████████████████████████| 23.2 MB 1.3 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.7/site-packages (from dask-ml) (1.21.2)\n","Collecting dask-glm>=0.2.0\n","  Using cached dask_glm-0.2.0-py2.py3-none-any.whl (12 kB)\n","Requirement already satisfied: distributed>=2.4.0 in /usr/local/lib/python3.7/site-packages (from dask-ml) (2021.9.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/site-packages (from dask-ml) (1.7.1)\n","Requirement already satisfied: dask[array,dataframe]>=2.4.0 in /usr/local/lib/python3.7/site-packages (from dask-ml) (2021.9.1)\n","Requirement already satisfied: multipledispatch>=0.4.9 in /usr/local/lib/python3.7/site-packages (from dask-ml) (0.6.0)\n","Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.7/site-packages (from dask-ml) (1.3.1)\n","Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.7/site-packages (from dask-ml) (0.53.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/site-packages (from dask-ml) (21.3)\n","Requirement already satisfied: cloudpickle>=0.2.2 in /usr/local/lib/python3.7/site-packages (from dask-glm>=0.2.0->dask-ml) (2.0.0)\n","Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.7/site-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (1.2.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/site-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (5.4.1)\n","Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.7/site-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (2021.11.1)\n","Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.7/site-packages (from dask[array,dataframe]>=2.4.0->dask-ml) (0.11.2)\n","Requirement already satisfied: tornado>=5 in /usr/local/lib/python3.7/site-packages (from distributed>=2.4.0->dask-ml) (6.1)\n","Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.7/site-packages (from distributed>=2.4.0->dask-ml) (1.0.2)\n","Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.7/site-packages (from distributed>=2.4.0->dask-ml) (2.4.0)\n","Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.7/site-packages (from distributed>=2.4.0->dask-ml) (1.7.0)\n","Requirement already satisfied: psutil>=5.0 in /usr/local/lib/python3.7/site-packages (from distributed>=2.4.0->dask-ml) (5.8.0)\n","Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.7/site-packages (from distributed>=2.4.0->dask-ml) (2.0.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (from distributed>=2.4.0->dask-ml) (49.6.0.post20210108)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/site-packages (from distributed>=2.4.0->dask-ml) (3.0.3)\n","Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.7/site-packages (from distributed>=2.4.0->dask-ml) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/site-packages (from multipledispatch>=0.4.9->dask-ml) (1.15.0)\n","Requirement already satisfied: llvmlite<0.37,>=0.36.0rc1 in /usr/local/lib/python3.7/site-packages (from numba>=0.51.0->dask-ml) (0.36.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/site-packages (from packaging->dask-ml) (3.0.6)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/site-packages (from pandas>=0.24.2->dask-ml) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/site-packages (from pandas>=0.24.2->dask-ml) (2021.3)\n","Requirement already satisfied: locket in /usr/local/lib/python3.7/site-packages (from partd>=0.3.10->dask[array,dataframe]>=2.4.0->dask-ml) (0.2.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/site-packages (from scikit-learn>=1.0.0->dask-ml) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/site-packages (from scikit-learn>=1.0.0->dask-ml) (3.0.0)\n","Requirement already satisfied: heapdict in /usr/local/lib/python3.7/site-packages (from zict>=0.1.3->distributed>=2.4.0->dask-ml) (1.0.1)\n","Requirement already satisfied: ipython in /usr/local/lib/python3.7/site-packages (from ipython-autotime) (7.30.1)\n","\u001b[33mWARNING: pymongo 4.0 does not provide the extra 'tls'\u001b[0m\n","Collecting dnspython<3.0.0,>=1.16.0\n","  Using cached dnspython-2.1.0-py3-none-any.whl (241 kB)\n","Collecting snuggs>=1.4.1\n","  Using cached snuggs-1.4.7-py3-none-any.whl (5.4 kB)\n","Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.7/site-packages (from rasterio) (0.7.2)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.7/site-packages (from rasterio) (2021.10.8)\n","Requirement already satisfied: attrs in /usr/local/lib/python3.7/site-packages (from rasterio) (21.2.0)\n","Collecting affine\n","  Using cached affine-2.3.0-py2.py3-none-any.whl (15 kB)\n","Requirement already satisfied: click-plugins in /usr/local/lib/python3.7/site-packages (from rasterio) (1.1.1)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/site-packages (from ipython->ipython-autotime) (0.7.5)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.7/site-packages (from ipython->ipython-autotime) (3.0.22)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.7/site-packages (from ipython->ipython-autotime) (0.1.3)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/site-packages (from ipython->ipython-autotime) (5.1.1)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/site-packages (from ipython->ipython-autotime) (5.1.0)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/site-packages (from ipython->ipython-autotime) (2.10.0)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/site-packages (from ipython->ipython-autotime) (4.8.0)\n","Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/site-packages (from ipython->ipython-autotime) (0.18.1)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.7/site-packages (from ipython->ipython-autotime) (0.2.0)\n","Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/site-packages (from jedi>=0.16->ipython->ipython-autotime) (0.8.3)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/site-packages (from pexpect>4.3->ipython->ipython-autotime) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->ipython-autotime) (0.2.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.7/site-packages (from jinja2->distributed>=2.4.0->dask-ml) (2.0.1)\n","Installing collected packages: scikit-learn, snuggs, pymongo, dnspython, dask-glm, affine, rasterio, ipython-autotime, dask-ml\n","  Attempting uninstall: scikit-learn\n","    Found existing installation: scikit-learn 0.24.2\n","    Uninstalling scikit-learn-0.24.2:\n","      Successfully uninstalled scikit-learn-0.24.2\n","Successfully installed affine-2.3.0 dask-glm-0.2.0 dask-ml-2021.11.30 dnspython-2.1.0 ipython-autotime-0.3.1 pymongo-4.0 rasterio-1.2.10 scikit-learn-1.0.1 snuggs-1.4.7\n","time: 139 µs (started: 2021-12-05 19:52:05 +00:00)\n"]}]},{"cell_type":"code","metadata":{"id":"qg2SasWKaBsB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638733931274,"user_tz":300,"elapsed":318,"user":{"displayName":"Ben Feuer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfEIGd7cuEAlEsO0rgrPNGu5w5YPlSFdk8U8iseac=s64","userId":"08479976376697798693"}},"outputId":"89412033-1270-4198-b6e6-063a78c61f26"},"source":["# This will install CondaColab.  This will restart your kernel one last time.  Run this cell by itself and only run the next cell once you see the session crash.\n","import condacolab\n","condacolab.install()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["✨🍰✨ Everything looks OK!\n","time: 12.1 ms (started: 2021-12-05 19:52:11 +00:00)\n"]}]},{"cell_type":"code","metadata":{"id":"fKSMDrN_aB-v","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638734076818,"user_tz":300,"elapsed":813,"user":{"displayName":"Ben Feuer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfEIGd7cuEAlEsO0rgrPNGu5w5YPlSFdk8U8iseac=s64","userId":"08479976376697798693"}},"outputId":"3d1b2ecf-3923-4631-e06d-a5410c89fb72"},"source":["# you can now run the rest of the cells as normal\n","import condacolab\n","condacolab.check()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["✨🍰✨ Everything looks OK!\n","time: 1.99 ms (started: 2021-12-05 19:54:36 +00:00)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lJddVazvk21t","executionInfo":{"status":"ok","timestamp":1638733911350,"user_tz":300,"elapsed":263,"user":{"displayName":"Ben Feuer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfEIGd7cuEAlEsO0rgrPNGu5w5YPlSFdk8U8iseac=s64","userId":"08479976376697798693"}},"outputId":"ed149be7-2781-4ac1-850d-cdc993a54833"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"m0jdXBRiDSzj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638733985482,"user_tz":300,"elapsed":47685,"user":{"displayName":"Ben Feuer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfEIGd7cuEAlEsO0rgrPNGu5w5YPlSFdk8U8iseac=s64","userId":"08479976376697798693"}},"outputId":"613b2d08-e2d1-4138-b705-d52212b325ac"},"source":["# Installing RAPIDS is now 'python rapidsai-csp-utils/colab/install_rapids.py <release> <packages>'\n","# The <release> options are 'stable' and 'nightly'.  Leaving it blank or adding any other words will default to stable.\n","# The <packages> option are default blank or 'core'.  By default, we install RAPIDSAI and BlazingSQL.  The 'core' option will install only RAPIDSAI and not include BlazingSQL, \n","!python rapidsai-csp-utils/colab/install_rapids.py stable\n","import os\n","os.environ['NUMBAPRO_NVVM'] = '/usr/local/cuda/nvvm/lib64/libnvvm.so'\n","os.environ['NUMBAPRO_LIBDEVICE'] = '/usr/local/cuda/nvvm/libdevice/'\n","os.environ['CONDA_PREFIX'] = '/usr/local'"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Installing RAPIDS Stable 21.10\n","Starting the RAPIDS install on Colab.  This will take about 15 minutes.\n","Collecting package metadata (current_repodata.json): ...working... done\n","Solving environment: ...working... WARNING conda.core.solve:_add_specs(611): pinned spec cudatoolkit=11.1 conflicts with explicit specs.  Overriding pinned spec.\n","done\n","\n","# All requested packages already installed.\n","\n","RAPIDS conda installation complete.  Updating Colab's libraries...\n","Copying /usr/local/lib/libcudf.so to /usr/lib/libcudf.so\n","Copying /usr/local/lib/libnccl.so to /usr/lib/libnccl.so\n","Copying /usr/local/lib/libcuml.so to /usr/lib/libcuml.so\n","Copying /usr/local/lib/libcugraph.so to /usr/lib/libcugraph.so\n","Copying /usr/local/lib/libxgboost.so to /usr/lib/libxgboost.so\n","Copying /usr/local/lib/libcuspatial.so to /usr/lib/libcuspatial.so\n","Copying /usr/local/lib/libgeos.so to /usr/lib/libgeos.so\n","Copying /usr/local/lib/libgeos_c.so to /usr/lib/libgeos_c.so\n","time: 47 s (started: 2021-12-05 19:52:18 +00:00)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HLvA4Ryvky-S","executionInfo":{"status":"ok","timestamp":1638733985780,"user_tz":300,"elapsed":5,"user":{"displayName":"Ben Feuer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfEIGd7cuEAlEsO0rgrPNGu5w5YPlSFdk8U8iseac=s64","userId":"08479976376697798693"}},"outputId":"12274560-12f1-4e4a-b276-a1a3839e233a"},"source":["import warnings\n","warnings.filterwarnings(\"ignore\")"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 1.12 ms (started: 2021-12-05 19:53:06 +00:00)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tf_ayoHEme4k","executionInfo":{"status":"ok","timestamp":1638733985779,"user_tz":300,"elapsed":306,"user":{"displayName":"Ben Feuer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfEIGd7cuEAlEsO0rgrPNGu5w5YPlSFdk8U8iseac=s64","userId":"08479976376697798693"}},"outputId":"9bc3fa68-1483-473e-d8c4-523cb8dac28b"},"source":["from pymongo import MongoClient\n","uri = \"mongodb+srv://baffree.pm9ae.mongodb.net/myFirstDatabase?authSource=%24external&authMechanism=MONGODB-X509&retryWrites=true&w=majority\"\n","client = MongoClient(uri,\n","                     tls=True,\n","                     tlsCertificateKeyFile='/content/drive/MyDrive/Vault/Mongo/X509-cert-7734489939085958648.pem')\n","db = client['bd']\n","collection = db['pt_xgb']"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 285 ms (started: 2021-12-05 19:53:05 +00:00)\n"]}]},{"cell_type":"code","metadata":{"id":"7D263TQDmh_W"},"source":["try:\n","  test_doc = {\"name\" : \"test\"}\n","  collection.insert_one(test_doc)\n","  print(\"insertion complete\")\n","  query = {\"name\": \"test\"}\n","  cursor = collection.find(query)\n","  for document in cursor:\n","        print(document)\n","  d = collection.delete_many(query)\n","  print(d.deleted_count, \" documents deleted !!\")\n","except:\n","  print(\"check mongoDB connection\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u7vehwOkomp4","executionInfo":{"status":"ok","timestamp":1638733991088,"user_tz":300,"elapsed":4006,"user":{"displayName":"Ben Feuer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfEIGd7cuEAlEsO0rgrPNGu5w5YPlSFdk8U8iseac=s64","userId":"08479976376697798693"}},"outputId":"c1b4fc6b-0840-400f-8758-b4faad4ea1db"},"source":["import sys\n","import os\n","import random\n","import time\n","import argparse\n","from time import time\n","from datetime import datetime\n","import pickle\n","from typing import Tuple\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","from sklearn import metrics\n","from sklearn.model_selection import cross_val_score\n","\n","import cv2\n","import rasterio\n","from rasterio.plot import show\n","from imutils import paths\n","import tqdm.notebook as tq\n","import imutils\n","\n","import dask\n","from dask.distributed import Client\n","import xgboost as xgb\n","from xgboost import dask as dxgb\n","from dask import dataframe as dd\n","import dask_cudf\n","from dask_cuda import LocalCUDACluster\n","from distributed import Client, wait\n","from dask_ml.model_selection import train_test_split\n","import pyarrow as pa\n","import cupy"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 3.82 s (started: 2021-12-05 19:53:07 +00:00)\n"]}]},{"cell_type":"markdown","metadata":{"id":"gaoyQqOHUZxa"},"source":["# Background Information on Dask and RAPIDS"]},{"cell_type":"markdown","metadata":{"id":"L6UyMYPxUdoK"},"source":["At this point, it's worth taking a few moments to explain the interrelated data formats we're using for this demonstration. Although they appear redundant, and in many ways they're intended to BE redundant, each of these DataFrame formats is its own object with its own set of defined methods, its own use case, living on its own type of device.\n","\n","[This](https://code-love.com/2020/12/06/rapids-introduction/) guide covers some, but not all, of the following comparisons --\n","\n","* **PANDAS**: Full functionality, great documentation and support. DataFrame lives in RAM, all functions run on 1 CPU, limited to 1 node.\n","* **DASK-DF**: Lazy evaluation extension of a Pandas DataFrame.\n","* **DASK ARRAY**: Lazy evaluation extension of a Numpy ndarray.\n","Both support multiple nodes, and multiple CPUs. The dataframe lives in RAM.\n","* **cuDF**: Real-time evaluation. Limited functionality and support. DataFrame lives in VRAM, all functions run in parallel on a single GPU, limited to 1 node.\n","* **Dask-cuDF**: Lazy evaluation extension of cuDF. The dataframe lives in VRAM, divided across an arbitrary number of GPUs and nodes. Quite limited functionality. Can handle arbitrarily large datasets, and will intelligently make use of RAM when VRAM runs out.\n","\n","The above is not a comprehensive list -- for instance, we aren't discussing Series, which are implemented in some degree on all of the above frameworks. In some of them, Series break down to ndarrays, and in others, they do not.\n","\n","Managing the interplay between these formats can be extremely tricky -- not only do we have to keep track of lazy evaluation, we also have to keep track of which device our data is stored on. This is a particular problem if we wish to go beyond the very limited set of built-in functions available in Dask on the CuDF itself -- going back and forth is extremely expensive, and it's something we only want to do once if we can possibly manage it.\n","\n","Like Spark, Dask uses the analogy of a computation graph, which it manages via Python's \"with\" context manager. This means that a Dask cluster is only 'alive' for a very limited period of time. This makes it challenging to use in an interactive environment, where we grow accustomed to having our variables 'hang around' for as long as we want them."]},{"cell_type":"markdown","metadata":{"id":"-5-3OHMfw0St"},"source":["# Image Classification: Ground-Level Fire Imagery"]},{"cell_type":"markdown","metadata":{"id":"n32qPT8KPeZZ"},"source":["For discussion of the datasets and models, please see the previous notebooks."]},{"cell_type":"markdown","metadata":{"id":"J1M4J8rzyDom"},"source":["## Setup"]},{"cell_type":"markdown","metadata":{"id":"fs5-OScyfQBr"},"source":["Distributed systems gain theoretical speed benefits when working from data that supports partitioning, like parquet files -- therefore, we are using them in this example, even though on a single system, this will probably result in a performance loss, rather than gain.\n","\n","The commented cells are for reference purposes only, to document how we create the parquet file from the original image data. We don't want these cells to run every time, because they take around 30 minutes."]},{"cell_type":"code","metadata":{"id":"nU3k7SwoxcRP"},"source":["class Config:\n","    # initialize the path to the fire and non-fire dataset directories\n","    FIRE_PATH = os.path.sep.join([\"/content/drive/MyDrive/datasets/Robbery_Accident_Fire_Database2\",\n","        \"Fire\"])\n","    NON_FIRE_PATH = \"/content/drive/MyDrive/datasets/spatial_envelope_256x256_static_8outdoorcategories\"\n","    \n","    # initialize the class labels in the dataset\n","    CLASSES = [\"Non-Fire\", \"Fire\"]\n","\n","    # define the size of the training and testing split\n","    TRAIN_SPLIT = 0.75\n","    TEST_SPLIT = 0.25\n","    MAX_SAMPLES = 1300\n","    CROSSVAL = 0\n","    \n","    # define the initial learning rate, batch size, and number of epochs\n","    INIT_LR = .01\n","    BATCH_SIZE = 64\n","    NUM_EPOCHS = 50\n","\n","    # set the path to the serialized model after training\n","    MODEL_PATH = os.path.sep.join([\"/content/drive/MyDrive/School/NYU/Big Data Fall 2021/Project/\", \"fire_detection.model\"])\n","    # set the path to the parquet file\n","    PARQUET_PATH = os.path.sep.join([\"/content/drive/MyDrive/datasets/\", \"groundfire.parquet\"])\n","    config.PARQUET_PATH\n","    # define the path to the output learning rate finder plot and\n","    # training history plot\n","    LRFIND_PLOT_PATH = os.path.sep.join([\"/content/drive/MyDrive/School/NYU/Big Data Fall 2021/Project/\", \"lrfind_plot.png\"])\n","    TRAINING_PLOT_PATH = os.path.sep.join([\"/content/drive/MyDrive/School/NYU/Big Data Fall 2021/Project/\", \"training_plot.png\"])\n","\n","    # define the path to the output directory that will store our final\n","    # output with labels/annotations along with the number of images to\n","    # sample\n","    OUTPUT_IMAGE_PATH = os.path.sep.join([\"/content/drive/MyDrive/School/NYU/Big Data Fall 2021/Project/\", \"examples\"])\n","    SAMPLE_SIZE = 50\n","\n","# initialize the configuration object\n","config = Config()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hUwnrhNExMIW"},"source":["# def load_dataset(datasetPath, shape, max_samples):\n","#   # grab the paths to all images in our dataset directory, then\n","#   # initialize our lists of images\n","#   imagePaths = list(paths.list_images(datasetPath))\n","#   data = []\n","\n","#   # loop over the image paths, limiting size of dataset to 1300 samples per class\n","#   count = 0\n","#   for imagePath in tq.tqdm(imagePaths):\n","#     count += 1\n","#     if count > max_samples:\n","#       print(\"breaking at {}\".format(max_samples))\n","#       break\n","#     # ignoring aspect ratio\n","#     image = cv2.imread(imagePath)\n","#     # load the image and resize it to be a fixed 128x128 pixels\n","#     image = cv2.resize(image, (128, 128))\n","#     if shape == \"row\":\n","#       image = np.asarray(image).flatten()\n","#     # add the image to the data lists\n","#     data.append(image)\n","\n","#   # return the data list as a NumPy array\n","#   np_data = np.array(data, dtype=\"float32\")\n","#   np_data /= 255\n","#   # Scale to [0,1]\n","#   return np_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qKK7rptkTQXt"},"source":["# fireData = load_dataset(config.FIRE_PATH, \"row\", config.MAX_SAMPLES)\n","# # print(fireData.shape)\n","# nonFireData = load_dataset(config.NON_FIRE_PATH, \"row\", config.MAX_SAMPLES)\n","# fireData = np.concatenate([fireData, np.ones((fireData.shape[0],1),dtype=fireData.dtype)], axis=1)\n","# nonFireData = np.concatenate([nonFireData, np.zeros((nonFireData.shape[0],1),dtype=nonFireData.dtype)], axis=1)\n","# data = np.vstack([fireData, nonFireData])\n","# # print(data.shape)\n","# #2600 rows, 49153 cols"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F0zCzitUPuZz"},"source":["Because of the way they are intended to function, parquet files have to be literal files -- we can't simply play around with them in memory as we can with, say, a Pandas DataFrame.\n","\n","Assembling a parquet file row by row from a series of Numpy arrays, as we do here, is an unconventional use of the format, to say the least, but for the purposes of demonstration, it's fine."]},{"cell_type":"code","metadata":{"id":"3lbLvcBJQULn"},"source":["# data_T = data.transpose()\n","# arrays = [\n","#   pa.array(col)  # Create one arrow array per column\n","#   for col in data_T\n","# ]\n","# table = pa.Table.from_arrays(\n","#     arrays,\n","#     names=['feature-{}'.format(i) for i in range(len(arrays)-1)]+[\"label\"] # give names to each columns\n","# )\n","# pa.parquet.write_table(table, 'groundfire.parquet')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2uSuL5dlYi_W"},"source":["## Save the model\n","\n","# from google.colab import files\n","\n","# files.download('groundfire.parquet')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NHcn6vr8FaCZ"},"source":["## Train and Test"]},{"cell_type":"markdown","metadata":{"id":"E_Tw2an_hxKX"},"source":["If we were running a true simulation, we would not reuse our validation data as test data. However, for the purposes of demonstrating the methodology, this is not a concern.\n","\n","Note that unlike our non-distributed training loop, this training loop makes extensive use of functions. This makes sense for distributed computing, which naturally lends itself to a functional programming paradigm. It also means we don't have to have a single ipynb cell with a zillion lines of unrelated code sitting inside of a context manager, thank goodness.\n","\n","In the cells below, you'll see Dask's persist() and wait() methods. These are extremely important. Wait is a typical async-await paradigm similar to what one might encounter when dealing with semaphores, parallel processing, or certain Javascript APIs -- but in this case, we are waiting for the various Dask workers to return their portions of the data before we proceed.\n","\n","persist() is a somewhat unconventional concept in Dask. A persist() object is not guaranteed to be available outside the context manager, but it is more likely to be available than it would be otherwise. It's cheaper than compute(), which is Dask's method for forcing evaluation of the graph, and serves as a kind of middle ground between no execution and complete execution of the graph."]},{"cell_type":"code","metadata":{"id":"vQeNB8idZQsT"},"source":["def load_groundfire(\n","    path,\n",") -> Tuple[\n","    dask_cudf.DataFrame, dask_cudf.Series, dask_cudf.DataFrame, dask_cudf.Series\n","]:\n","    df = dask_cudf.read_parquet(path)\n","\n","    y = df[\"label\"]\n","    X = df[df.columns.difference([\"label\"])]\n","\n","    X_train, X_valid, y_train, y_valid = train_test_split(\n","        X, y, test_size=0.25, shuffle=True\n","    )\n","    X_train, X_valid, y_train, y_valid = client.persist(\n","        [X_train, X_valid, y_train, y_valid]\n","    )\n","    wait([X_train, X_valid, y_train, y_valid])\n","\n","    return X_train, X_valid, y_train, y_valid"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Knclb-DfVzAk"},"source":["A version of XGBoost is actually implemented within Dask itself -- that's the dxgb we see in the below function. Note that dxgb is responsible for storing the data, since it understands that it may be distributed, which conventional XGBoost would not, of course.\n","\n","The train function is using a special tree method here, gpu_hist, designed to take advantage of the fact that the data is stored on a GPU."]},{"cell_type":"code","metadata":{"id":"n-s7iH7rgdSQ"},"source":["def fit_model_customized_es(client, X, y, X_valid, y_valid):\n","    early_stopping_rounds = 10\n","\n","    es = xgb.callback.EarlyStopping(rounds=early_stopping_rounds, save_best=True)\n","\n","    Xy = dxgb.DaskDeviceQuantileDMatrix(client, X, y)\n","\n","    Xy_valid = dxgb.DaskDMatrix(client, X_valid, y_valid)\n","\n","    booster = xgb.dask.train(\n","        client,\n","        {\n","            \"objective\": \"binary:logistic\",\n","            \"eval_metric\": \"error\",\n","            \"tree_method\": \"gpu_hist\",\n","        },\n","        Xy,\n","        evals=[(Xy_valid, \"Valid\")],\n","        num_boost_round=1000,\n","        callbacks=[es],\n","    )[\"booster\"]\n","    return booster"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j4uBgoqqg0a1"},"source":["def explain(client, model, X):\n","   # Use array instead of dataframe in case of output dim is greater than 2.\n","   X_array = X.values\n","   contribs = dxgb.predict(\n","       client, model, X_array, pred_contribs=True, validate_features=False\n","   )\n","   # Use the result for further analysis\n","   return contribs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2U6XnIhLg0a1"},"source":["def predict(client, model, X):\n","    predt = dxgb.predict(client, model, X)\n","    assert isinstance(predt, dd.Series)\n","    return predt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_PevznKAnRTx"},"source":["def save_results(expected_y, predicted_y, contribs, start_time, config):\n","  # make predictions\n","\n","  cur_time = datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")\n","\n","  keystr = \"xgb_ground_dist_\" + cur_time\n","\n","  classif = metrics.classification_report(expected_y, predicted_y)\n","\n","  conf = metrics.confusion_matrix(expected_y, predicted_y)\n","\n","  rtime = time() - start_time\n","\n","  results = {\"name\" : keystr, \"classifier\" : \"xgboost\", \"dataset\" : \"ground\", \"distributed\" : True, \"metric\" : classif, \"confmatrix\" : conf.tolist(), \"runtime\" : rtime, \"train_test_split\" : config.TEST_SPLIT, \"cross_val\" : config.CROSSVAL}\n","\n","  #store in mongoDB\n","\n","  collection.insert_one(results)\n","\n","  #store to local pickle\n","\n","  PICKLE_FILE = keystr + \".pkl\"\n","\n","  PICKLE_PATH = os.path.join(PATH, PICKLE_FILE)\n","\n","  with open(PICKLE_PATH, 'wb') as handle:\n","      pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","  # summarize the fit of the model\n","  print(); print(classif)\n","  print(); print(conf)\n","\n","  print()\n","  print(\"Execution Time %s seconds: \" % (rtime)) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"61Znh5Sk01Kj"},"source":["The cell below preserves pointers to the model, dataframes and results locally.\n","\n","In a true distributed environment (IE, in production), we would perform all operations inside Dask's context manager so that the load could be distributed over multiple nodes. But in Colab, we only have one node, and doing this allows us to inspect and manipulate the data after the context manager has run (or even if it crashes!)\n","\n","One thing we cannot do is alter the dataframes -- if we do, Dask will know and produce an error --\n","\n","```\n","Inputs contain futures that were created by another client\n","```\n","\n","A comment on timing -- it's interesting to note that the data-processing runs 4-5x slower in Dask/CuDF than it does in SKLearn/Numpy. Distributing data only saves time in a true distributed environment."]},{"cell_type":"code","metadata":{"id":"pGkPtl4J0aoD"},"source":["X_train, X_valid, y_train, y_valid, booster, preds, contribs = None, None, None, None, None, None, None"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fbJ3sMQNiQXh","outputId":"b8125c24-3ab8-419e-95b1-95067f7c5aa9"},"source":["if __name__ == \"__main__\":\n","   start_time = time()\n","   with LocalCUDACluster() as cluster:\n","       print(\"dashboard:\", cluster.dashboard_link)\n","       with Client(cluster) as client:\n","          print(\"Load data ...\")\n","          X_train, X_valid, y_train, y_valid = load_groundfire(config.PARQUET_PATH)\n","          print(\"Load complete.\")\n","          print(\"Begin model training...\")\n","          booster = fit_model_customized_es(client, X_train, y_train, X_valid, y_valid)\n","          print(\"Training complete.\")\n","          preds = predict(client, booster, X_valid)\n","          #print(\"type of preds and yvalid\")\n","          #print(type(preds)) #dask.dataframe.core.series\n","          # print(type(y_valid)) #dask_cudf.core.series\n","          preds_np = cupy.asnumpy(preds.to_dask_array()) \n","          preds_np = np.rint(preds_np)\n","          # print(\"type of preds_np and y_valid_df\")\n","          #print(type(preds_np)) #numpy array\n","          #y_valid_df = y_valid.compute() #cudf.core.series.Series\n","          # print(type(y_valid_df)) \n","          #y_valid_np = cupy.asnumpy(y_valid_df)\n","          #y_valid_np = y_valid.compute().values #cupy._core.core.ndarray\n","          y_valid_np = cupy.asnumpy(y_valid.compute().values)\n","          # print(type(y_valid_np))\n","          contribs = explain(client, booster, X_train)\n","          contribs_np = cupy.asnumpy(contribs)\n","          contribs_list = contribs_np.tolist()\n","          save_results(y_valid_np, preds_np, contribs_list, start_time, config)\n","          # print(\"---METRICS---\"); print(metrics.classification_report(y_valid_np, preds_np))\n","          # print(\"---CONFUSION MATRIX---\"); print(metrics.confusion_matrix(y_valid_np, preds_np))\n","          # print(\"---CONTRIBUTIONS TO PREDICTION---\"); print(contribs)\n","   print(\"Execution Time %s seconds: \" % (time() - start_time))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["dashboard: http://127.0.0.1:8787/status\n","Load data ...\n","Load complete.\n","Begin model training...\n","[0]\tValid-error:0.29100\n","[1]\tValid-error:0.27010\n","[2]\tValid-error:0.25723\n","[3]\tValid-error:0.25884\n","[4]\tValid-error:0.25402\n","[5]\tValid-error:0.24437\n","[6]\tValid-error:0.24598\n","[7]\tValid-error:0.25080\n","[8]\tValid-error:0.23633\n","[9]\tValid-error:0.24920\n","[10]\tValid-error:0.23794\n","[11]\tValid-error:0.24277\n","[12]\tValid-error:0.22669\n","[13]\tValid-error:0.22187\n","[14]\tValid-error:0.21383\n","[15]\tValid-error:0.21704\n","[16]\tValid-error:0.21543\n","[17]\tValid-error:0.21061\n","[18]\tValid-error:0.21543\n","[19]\tValid-error:0.20900\n","[20]\tValid-error:0.21383\n","[21]\tValid-error:0.21383\n","[22]\tValid-error:0.20900\n","[23]\tValid-error:0.20579\n","[24]\tValid-error:0.21383\n","[25]\tValid-error:0.20579\n","[26]\tValid-error:0.19936\n","[27]\tValid-error:0.19453\n","[28]\tValid-error:0.19293\n","[29]\tValid-error:0.20739\n","[30]\tValid-error:0.20097\n","[31]\tValid-error:0.20257\n","[32]\tValid-error:0.20257\n","[33]\tValid-error:0.20257\n","[34]\tValid-error:0.20579\n","[35]\tValid-error:0.19936\n","[36]\tValid-error:0.19775\n","[37]\tValid-error:0.19936\n","Training complete.\n"]}]},{"cell_type":"markdown","metadata":{"id":"OKhcbM9IEwDY"},"source":["# Image Classification: LANDSAT-8 Imagery"]},{"cell_type":"markdown","metadata":{"id":"efHstlmQE-87"},"source":["## Setup"]},{"cell_type":"markdown","metadata":{"id":"-gp7I60NahWf"},"source":["The commented cells are for reference purposes only, to document how I created the parquet file from the original image data. We don't want these cells to run every time, because they take around 30 minutes."]},{"cell_type":"code","metadata":{"id":"wskquo_NMmTT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638735197665,"user_tz":300,"elapsed":271,"user":{"displayName":"Ben Feuer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfEIGd7cuEAlEsO0rgrPNGu5w5YPlSFdk8U8iseac=s64","userId":"08479976376697798693"}},"outputId":"eff844a0-3333-4819-91e9-0acc1379407b"},"source":["class Config:\n","    # initialize the path to the fire and non-fire dataset directories\n","    FIRE_PATH = os.path.sep.join([\"/content/drive/MyDrive/datasets/landsat_mini/Training\",\n","        \"Fire\"])\n","    NON_FIRE_PATH = os.path.sep.join([\"/content/drive/MyDrive/datasets/landsat_mini/Training\", \"No_Fire\"])\n","    \n","    # initialize the class labels in the dataset\n","    CLASSES = [\"Non-Fire\", \"Fire\"]\n","\n","    # define the size of the training and testing split\n","    TRAIN_SPLIT = 0.75\n","    TEST_SPLIT = 0.25\n","    MAX_SAMPLES = 1300\n","    CROSSVAL = 0\n","\n","    # set the path to the serialized model after training\n","    MODEL_PATH = os.path.sep.join([\"/content/drive/MyDrive/School/NYU/Big Data Fall 2021/Project/\", \"fire_detection_xgb_landsat.model\"])\n","\n","    PARQUET_PATH = os.path.sep.join([\"/content/drive/MyDrive/datasets/\", \"landsatfire.parquet\"])\n","\n","    PATH = r\"/content/drive/MyDrive/School/NYU/Big Data Fall 2021/Project\"\n","    \n","    # define the path to the output learning rate finder plot and\n","    # training history plot\n","    LRFIND_PLOT_PATH = os.path.sep.join([\"/content/drive/MyDrive/School/NYU/Big Data Fall 2021/Project/\", \"lrfind_plot_xgb_landsat.png\"])\n","    TRAINING_PLOT_PATH = os.path.sep.join([\"/content/drive/MyDrive/School/NYU/Big Data Fall 2021/Project/\", \"training_plot_xgb_landsat.png\"])\n","\n","    # define the path to the output directory that will store our final\n","    # output with labels/annotations along with the number of images to\n","    # sample\n","    OUTPUT_IMAGE_PATH = os.path.sep.join([\"/content/drive/MyDrive/School/NYU/Big Data Fall 2021/Project/\", \"examples\"])\n","    SAMPLE_SIZE = 50\n","\n","# initialize the configuration object\n","config = Config()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 19.2 ms (started: 2021-12-05 20:13:18 +00:00)\n"]}]},{"cell_type":"code","metadata":{"id":"hLvUdapxMmTW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638734089154,"user_tz":300,"elapsed":26,"user":{"displayName":"Ben Feuer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfEIGd7cuEAlEsO0rgrPNGu5w5YPlSFdk8U8iseac=s64","userId":"08479976376697798693"}},"outputId":"d5d52dac-6dfe-4b9e-8b7a-39b664643ed8"},"source":["# def load_dataset(datasetPath, shape, max_samples):\n","#   # grab the paths to all images in our dataset directory, then\n","#   # initialize our lists of images\n","#   imagePaths = list(paths.list_images(datasetPath))\n","#   data = []\n","\n","#   # loop over the image paths, limiting size of dataset to 1300 samples per class\n","#   count = 0\n","#   for imagePath in tq.tqdm(imagePaths):\n","#     count += 1\n","#     if count > max_samples:\n","#       print(\"breaking at {}\".format(max_samples))\n","#       break\n","#     # ignoring aspect ratio\n","#     img = rasterio.open(imagePath)\n","#     image = rasterio.plot.reshape_as_image(img.read((2,3,7)))\n","#     if shape == \"row\":\n","#       image = image.flatten()\n","#     # add the image to the data lists\n","#     data.append(image)\n","\n","#   # return the data list as a NumPy array\n","#   np_data = np.array(data, dtype=\"float32\")\n","#   # print(np_data.shape)\n","#   # Scale to [0,1]\n","#   np_data /= (np_data.max()-np_data.min())\n","#   return np_data"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 3.84 ms (started: 2021-12-05 19:54:49 +00:00)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DPUhFffiGVJz","executionInfo":{"status":"ok","timestamp":1638734089155,"user_tz":300,"elapsed":25,"user":{"displayName":"Ben Feuer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfEIGd7cuEAlEsO0rgrPNGu5w5YPlSFdk8U8iseac=s64","userId":"08479976376697798693"}},"outputId":"ec7916d0-f84e-46ee-a427-bf3680bff7de"},"source":["# fireData = load_dataset(config.FIRE_PATH, \"row\", config.MAX_SAMPLES)\n","# # print(fireData.shape)\n","# nonFireData = load_dataset(config.NON_FIRE_PATH, \"row\", config.MAX_SAMPLES)\n","# fireData = np.concatenate([fireData, np.ones((fireData.shape[0],1),dtype=fireData.dtype)], axis=1)\n","# nonFireData = np.concatenate([nonFireData, np.zeros((nonFireData.shape[0],1),dtype=nonFireData.dtype)], axis=1)\n","# data = np.vstack([fireData, nonFireData])\n","# # print(data.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 1.55 ms (started: 2021-12-05 19:54:49 +00:00)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EyQI4JJ6GVJ0","executionInfo":{"status":"ok","timestamp":1638734089155,"user_tz":300,"elapsed":23,"user":{"displayName":"Ben Feuer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfEIGd7cuEAlEsO0rgrPNGu5w5YPlSFdk8U8iseac=s64","userId":"08479976376697798693"}},"outputId":"5501149c-43af-4f99-b8d6-6a00915993e0"},"source":["# data_T = data.transpose()\n","# arrays = [\n","#   pa.array(col)  # Create one arrow array per column\n","#   for col in data_T\n","# ]\n","# table = pa.Table.from_arrays(\n","#     arrays,\n","#     names=['feature-{}'.format(i) for i in range(len(arrays)-1)]+[\"label\"] # give names to each columns\n","# )\n","# pa.parquet.write_table(table, 'landsatfire.parquet')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 1.7 ms (started: 2021-12-05 19:54:49 +00:00)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RLWu5gNYGVJ0","executionInfo":{"status":"ok","timestamp":1638734089156,"user_tz":300,"elapsed":21,"user":{"displayName":"Ben Feuer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfEIGd7cuEAlEsO0rgrPNGu5w5YPlSFdk8U8iseac=s64","userId":"08479976376697798693"}},"outputId":"abf0ac01-6c30-4d1c-f004-a5b29d90ce4e"},"source":["## Debug cells\n","\n","# !ls -lh groundfire.parquet\n","# df = dask_cudf.read_parquet('groundfire.parquet')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 1.08 ms (started: 2021-12-05 19:54:49 +00:00)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pr83alnvGVJ1","executionInfo":{"status":"ok","timestamp":1638734089156,"user_tz":300,"elapsed":20,"user":{"displayName":"Ben Feuer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfEIGd7cuEAlEsO0rgrPNGu5w5YPlSFdk8U8iseac=s64","userId":"08479976376697798693"}},"outputId":"df6a01d2-d6f0-4bba-9887-b5f006fb1bc4"},"source":["# # Save the model\n","\n","# from google.colab import files\n","\n","# files.download('landsatfire.parquet')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 1.19 ms (started: 2021-12-05 19:54:49 +00:00)\n"]}]},{"cell_type":"markdown","metadata":{"id":"VaXhVz0ZGBR3"},"source":["## Train and Test"]},{"cell_type":"markdown","metadata":{"id":"8lmyN_sbGBR4"},"source":["If we were running a true simulation, we would not reuse our validation data as test data. However, for the purposes of demonstrating the methodology, this is not a concern, so long as we remember that the accuracy values here are definitely NOT accurate."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7FtIWoqoGBR5","executionInfo":{"status":"ok","timestamp":1638735225248,"user_tz":300,"elapsed":672,"user":{"displayName":"Ben Feuer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfEIGd7cuEAlEsO0rgrPNGu5w5YPlSFdk8U8iseac=s64","userId":"08479976376697798693"}},"outputId":"e88f3994-ef01-457e-e258-86d08096650f"},"source":["def load_groundfire(\n","    path,\n",") -> Tuple[\n","    dask_cudf.DataFrame, dask_cudf.Series, dask_cudf.DataFrame, dask_cudf.Series\n","]:\n","    df = dask_cudf.read_parquet(path)\n","\n","    y = df[\"label\"]\n","    X = df[df.columns.difference([\"label\"])]\n","\n","    X_train, X_valid, y_train, y_valid = train_test_split(\n","        X, y, test_size=0.25, shuffle=True\n","    )\n","    X_train, X_valid, y_train, y_valid = client.persist(\n","        [X_train, X_valid, y_train, y_valid]\n","    )\n","    wait([X_train, X_valid, y_train, y_valid])\n","\n","    return X_train, X_valid, y_train, y_valid"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 8.75 ms (started: 2021-12-05 20:13:45 +00:00)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QlJXvsPZGBR5","executionInfo":{"status":"ok","timestamp":1638735225823,"user_tz":300,"elapsed":8,"user":{"displayName":"Ben Feuer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfEIGd7cuEAlEsO0rgrPNGu5w5YPlSFdk8U8iseac=s64","userId":"08479976376697798693"}},"outputId":"86f2f2a0-652d-4871-bd4e-ed7ebc04c126"},"source":["def fit_model_customized_es(client, X, y, X_valid, y_valid):\n","    early_stopping_rounds = 10\n","\n","    es = xgb.callback.EarlyStopping(rounds=early_stopping_rounds, save_best=True)\n","\n","    Xy = dxgb.DaskDeviceQuantileDMatrix(client, X, y)\n","\n","    Xy_valid = dxgb.DaskDMatrix(client, X_valid, y_valid)\n","\n","    booster = xgb.dask.train(\n","        client,\n","        {\n","            \"objective\": \"binary:logistic\",\n","            \"eval_metric\": \"error\",\n","            \"tree_method\": \"gpu_hist\",\n","        },\n","        Xy,\n","        evals=[(Xy_valid, \"Valid\")],\n","        num_boost_round=1000,\n","        callbacks=[es],\n","    )[\"booster\"]\n","    return booster"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 7.12 ms (started: 2021-12-05 20:13:45 +00:00)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A77vdwpQGBR6","executionInfo":{"status":"ok","timestamp":1638735225824,"user_tz":300,"elapsed":6,"user":{"displayName":"Ben Feuer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfEIGd7cuEAlEsO0rgrPNGu5w5YPlSFdk8U8iseac=s64","userId":"08479976376697798693"}},"outputId":"6e25d6c5-cfde-4997-df38-18a1b3de5aca"},"source":["def explain(client, model, X):\n","   # Use array instead of dataframe in case of output dim is greater than 2.\n","   X_array = X.values\n","   contribs = dxgb.predict(\n","       client, model, X_array, pred_contribs=True, validate_features=False\n","   )\n","   # Use the result for further analysis\n","   return contribs"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 3 ms (started: 2021-12-05 20:13:45 +00:00)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UmbfupOlGBR6","executionInfo":{"status":"ok","timestamp":1638735225824,"user_tz":300,"elapsed":5,"user":{"displayName":"Ben Feuer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfEIGd7cuEAlEsO0rgrPNGu5w5YPlSFdk8U8iseac=s64","userId":"08479976376697798693"}},"outputId":"055d6b24-6805-4540-9ad4-9f4725accbef"},"source":["def predict(client, model, X):\n","    predt = dxgb.predict(client, model, X)\n","    assert isinstance(predt, dd.Series)\n","    return predt"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 2 ms (started: 2021-12-05 20:13:45 +00:00)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wtwbSjSj2bC5","executionInfo":{"status":"ok","timestamp":1638735225825,"user_tz":300,"elapsed":6,"user":{"displayName":"Ben Feuer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfEIGd7cuEAlEsO0rgrPNGu5w5YPlSFdk8U8iseac=s64","userId":"08479976376697798693"}},"outputId":"6a69333c-438d-422b-95e9-2e5afc29e620"},"source":["def save_results(expected_y, predicted_y, contribs, start_time, config):\n","  # make predictions\n","\n","  cur_time = datetime.now().strftime(\"%d_%m_%Y_%H_%M_%S\")\n","\n","  keystr = \"xgb_sat_dist_\" + cur_time\n","\n","  classif = metrics.classification_report(expected_y, predicted_y)\n","\n","  conf = metrics.confusion_matrix(expected_y, predicted_y)\n","\n","  rtime = time() - start_time\n","\n","  results = {\"name\" : keystr, \"classifier\" : \"xgboost\", \"dataset\" : \"satellite\", \"distributed\" : True, \"metric\" : classif, \"confmatrix\" : conf.tolist(), \"runtime\" : rtime, \"train_test_split\" : config.TEST_SPLIT, \"cross_val\" : config.CROSSVAL}\n","\n","  #store in mongoDB\n","\n","  collection.insert_one(results)\n","\n","  #store to local pickle\n","\n","  PICKLE_FILE = keystr + \".pkl\"\n","\n","  PICKLE_PATH = os.path.join(config.PATH, PICKLE_FILE)\n","\n","  with open(PICKLE_PATH, 'wb') as handle:\n","      pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","  # summarize the fit of the model\n","  print(); print(classif)\n","  print(); print(conf)\n","\n","  print()\n","  print(\"Execution Time %s seconds: \" % (rtime)) "],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 24.4 ms (started: 2021-12-05 20:13:45 +00:00)\n"]}]},{"cell_type":"markdown","metadata":{"id":"yVLb5YRUGBR7"},"source":["The cell below preserves pointers to the model, dataframes and results locally.\n","\n","In a true distributed environment (IE, in production), we would perform all operations inside Dask's context manager so that the load could be distributed over multiple nodes. But in Colab, we only have one node, and doing this allows us to inspect and manipulate the data after the context manager has run (or even if it crashes!)\n","\n","One thing we cannot do is alter the dataframes -- if we do, Dask will know and produce an error --\n","\n","```\n","Inputs contain futures that were created by another client\n","```\n","\n","A comment on timing -- it's interesting to note that the data-processing runs 4-5x slower in Dask/CuDF than it does in SKLearn/Numpy. Distributing data only saves time in a true distributed environment."]},{"cell_type":"code","metadata":{"id":"GLD-IS_MGBR7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638735225825,"user_tz":300,"elapsed":5,"user":{"displayName":"Ben Feuer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfEIGd7cuEAlEsO0rgrPNGu5w5YPlSFdk8U8iseac=s64","userId":"08479976376697798693"}},"outputId":"3a79b109-bd27-4a61-9008-d00b397cf76e"},"source":["X_train, X_valid, y_train, y_valid, booster, preds, contribs = None, None, None, None, None, None, None"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["time: 829 µs (started: 2021-12-05 20:13:45 +00:00)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rhWsgsmiDIoC","executionInfo":{"status":"ok","timestamp":1638735561811,"user_tz":300,"elapsed":335990,"user":{"displayName":"Ben Feuer","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgfEIGd7cuEAlEsO0rgrPNGu5w5YPlSFdk8U8iseac=s64","userId":"08479976376697798693"}},"outputId":"a296e452-3ab0-4c8f-ea57-1c98849e1bf0"},"source":["if __name__ == \"__main__\":\n","   start_time = time()\n","   with LocalCUDACluster() as cluster:\n","       print(\"dashboard:\", cluster.dashboard_link)\n","       with Client(cluster) as client:\n","          print(\"Load data ...\")\n","          X_train, X_valid, y_train, y_valid = load_groundfire(config.PARQUET_PATH)\n","          print(\"Load complete.\")\n","          print(\"Begin model training...\")\n","          booster = fit_model_customized_es(client, X_train, y_train, X_valid, y_valid)\n","          print(\"Training complete.\")\n","          preds = predict(client, booster, X_valid)\n","          #print(\"type of preds and yvalid\")\n","          #print(type(preds)) #dask.dataframe.core.series\n","          #print(type(y_valid)) #dask_cudf.core.series\n","          preds_np = cupy.asnumpy(preds.to_dask_array()) \n","          preds_np = np.rint(preds_np) #round predictions to nearest integer (0,1)\n","          #print(\"type of preds_np and y_valid_df\")\n","          #print(type(preds_np)) #numpy array\n","          y_valid_np = cupy.asnumpy(y_valid.compute().values)\n","          # print(type(y_valid_np)) #numpy array\n","          contribs = explain(client, booster, X_train)\n","          # contribs_np = cupy.asnumpy(contribs)\n","          # contribs_list = contribs_np.tolist()\n","          save_results(y_valid_np, preds_np, contribs, start_time, config)\n","          # contribs = contribs.compute().as_matrix()\n","          # print(\"---METRICS---\"); print(metrics.classification_report(y_valid_np, preds_np))\n","          # print(\"---CONFUSION MATRIX---\"); print(metrics.confusion_matrix(y_valid_np, preds_np))\n","          # print(\"---CONTRIBUTIONS TO PREDICTION---\"); print(contribs)\n","   print(\"Execution Time %s seconds: \" % (time() - start_time))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["dashboard: http://127.0.0.1:8787/status\n","Load data ...\n","Load complete.\n","Begin model training...\n","[0]\tValid-error:0.16667\n","[1]\tValid-error:0.05556\n","[2]\tValid-error:0.10000\n","[3]\tValid-error:0.03333\n","[4]\tValid-error:0.03333\n","[5]\tValid-error:0.04444\n","[6]\tValid-error:0.03333\n","[7]\tValid-error:0.03333\n","[8]\tValid-error:0.03333\n","[9]\tValid-error:0.03333\n","[10]\tValid-error:0.03333\n","[11]\tValid-error:0.03333\n","[12]\tValid-error:0.03333\n","Training complete.\n","\n","              precision    recall  f1-score   support\n","\n","         0.0       0.93      0.96      0.95        27\n","         1.0       0.98      0.97      0.98        63\n","\n","    accuracy                           0.97        90\n","   macro avg       0.96      0.97      0.96        90\n","weighted avg       0.97      0.97      0.97        90\n","\n","\n","[[26  1]\n"," [ 2 61]]\n","\n","Execution Time 335.193058013916 seconds: \n","Execution Time 336.52417731285095 seconds: \n","time: 5min 36s (started: 2021-12-05 20:13:45 +00:00)\n"]}]},{"cell_type":"markdown","metadata":{"id":"s88i61HALTSd"},"source":["# Conclusions"]},{"cell_type":"markdown","metadata":{"id":"Amr6oCYuLVWr"},"source":["While distributed training of XGBoost models on Dask is not quite as intuitive as the vanilla framework, its deep integration with Dask's architecture and ample documentation make it a very reasonable learning curve.\n","\n","Even in this artificial setting, it's easy to see how the performance gains enabled by distributed computing will quickly become very desirable, and probably even commonplace at some point."]}]}